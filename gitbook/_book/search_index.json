[
["index.html", "dispRity manual 1 dispRity 1.1 What is dispRity? 1.2 Installing and running the package 1.3 Why not CRAN? 1.4 Help 1.5 Citations", " dispRity manual Thomas Guillerme (guillert@tcd.ie) and Natalie Cooper (natalie.cooper@nhm.ac.uk) 2017-08-12 1 dispRity This is a package for measuring disparity in R. It allows users to summarise ordinated matrices (e.g. MDS, PCA, PCO, PCoA) into single values so they can easily be compared. This manual is based on the version 1.0. 1.1 What is dispRity? This is a package for measuring disparity in R. It allows users to summarise ordinated matrices (e.g. MDS, PCA, PCO, PCoA) to perform some multidimensional analysis. Typically, these analysis are used in palaeobiology and evolutionary biology to study the changes in morphology through time. However, there are many more applications in ecology, evolution and beyond. 1.2 Installing and running the package You can install this package easily if you use the latest version of R (&gt; 4.0) and devtools. ## Checking if devtools is already installed if(!require(devtools)) install.packages(&quot;devtools&quot;) ## Installing the latest released version directly from GitHub install_github(&quot;TGuillerme/dispRity&quot;, ref = &quot;release&quot;) ## Loading the package library(dispRity) Note this uses the release branch (version0.4). For the piping-hot (but potentially unstable) version, you can change the argument ref = release to ref = master. dispRity depends mainly on the ape package and uses functions from several other packages (ade4, geometry, grDevices, hypervolume, paleotree, snow, Claddis, geomorph and RCurl). 1.3 Why not CRAN? This package is not available on CRAN. This is mainly because some parts are still in development and that the reactivity of GitHub is better for implementing new suggestions from users. However, the package follows the strict (and useful!) CRAN standards via Travis. 1.4 Help If you need help with the package, hopefully the following manual will be useful. However, parts of this package are still in development and some other parts are probably not covered. Thus if you have suggestions or comments on on what has already been developed or will be developed, please send me an email (guillert@tcd.ie) or if you are a GitHub user, directly create an issue on the GitHub page. 1.5 Citations You can cite both the package or this manual with the following citation: Guillerme, T. (2016). dispRity: a package for measuring disparity in R. Zenodo. 10.5281/zenodo.55646 Note that this citation is only temporary (but can still be used!). A future proper version of the latest package release, this manual and an associated methods paper should be submitted soon(ish!). "],
["glossary.html", "2 Glossary", " 2 Glossary Ordinated space. The mathematical multidimensional object that will be analysed with this package. In morphometrics, this is often referred to as the morphospace. However it may also be referred to as the cladisto-space for cladistic data or the eco-space for ecological data etc. In practice, this term designates an ordinated matrix where the columns represent the dimensions of the space (often – but not necessarily - &gt; 3!) and the rows represent the elements within this space. Elements. The rows of the ordinated space. Elements can be taxa, field sites, countries etc. Dimensions. The columns of the ordinated space. The dimensions are referred to as axes of variation, or principal components, for ordinated spaces obtained from a PCA. Subsamples. Sub-samples of the ordinated space. A sub-sample (or subsamples) contains the same number of dimensions as the ordinated space but may contain a smaller subset of elements. For example, if our ordinated space is composed of birds and mammals (the elements) and 50 principal components (the dimensions), we can create two subsamples containing just mammals or birds, but with the same 50 dimensions, to compare disparity in the two clades. "],
["getting-started-with-disprity.html", "3 Getting started with dispRity 3.1 What sort of data does dispRity work with? 3.2 Performing a simple dispRity analysis", " 3 Getting started with dispRity 3.1 What sort of data does dispRity work with? Disparity can be estimated from pretty much any matrix. Classically, however, it is measured from ordinated matrices. These matrices can be from any type of ordination (PCO, PCA, PCoA, MDS, etc.) as long as they have your element names as rows (taxa, experiments, countries etc.) and your ordination axes as columns (the dimensions of your dataset). 3.1.1 Ordination matrices from Claddis dispRity package can easily take data from Claddis using the Claddis.ordination function. For this, simply input a matrix in the Claddis format to the function and it will automatically calculate and ordinate the distances among taxa: require(Claddis) ## Ordinating the example data from Claddis Claddis.ordination(Michaux1989) ## [,1] [,2] [,3] ## Ancilla 7.252259e-17 4.154578e-01 0.2534942 ## Turrancilla -5.106645e-01 -4.566150e-16 -0.2534942 ## Ancillista 5.106645e-01 -8.153839e-16 -0.2534942 ## Amalda -3.207162e-16 -4.154578e-01 0.2534942 Note that several options are available, namely which type of distance should be computed. See more info in the function manual (?Claddis.ordination). Alternatively, it is of course also possible to manual calculate the ordination matrix using the functions Claddis::MorphDistMatrix and stats::cmdscale. 3.1.2 Ordination matrices from geomorph You can also easily use data from geomorph using the geomorph.ordination function. This function simply takes Procrustes aligned data and performs an ordination: require(geomorph) ## Loading the plethodon dataset data(plethodon) ## Performing a Procrustes transform on the landmarks procrustes &lt;- gpagen(plethodon$land, PrinAxes = FALSE, print.progress = FALSE) ## Ordinating this data geomorph.ordination(procrustes)[1:5,1:5] ## PC1 PC2 PC3 PC4 PC5 ## [1,] -0.0369931363 0.05118247 -0.0016971082 -0.003128809 -0.010936371 ## [2,] -0.0007493738 0.05942082 0.0001371715 -0.002768680 -0.008117383 ## [3,] 0.0056004654 0.07419599 -0.0052612103 -0.005034566 -0.002746592 ## [4,] -0.0134808572 0.06463959 -0.0458436015 -0.007887369 0.009816827 ## [5,] -0.0334696244 0.06863518 0.0136292041 0.007359409 0.022347225 Options for the ordination (from ?prcomp) can be directly passed to this function to perform customised ordinations. Additionally you can give the function a geomorph.data.frame object. If the latter contains sorting information (i.e. factors), they can be directly used to make a customised dispRity object customised dispRity object! ## Using a geomorph.data.frame geomorph_df &lt;- geomorph.data.frame(procrustes, species = plethodon$species, site = plethodon$site) ## Ordinating this data and making a dispRity object geomorph.ordination(geomorph_df) ## ---- dispRity object ---- ## 4 customised subsamples for 40 elements: ## species.Jord, species.Teyah, site.Allo, site.Symp. More about these dispRity objects below! 3.1.3 Other kinds of ordination matrices If you are not using the packages mentioned above (Claddis and geomorph) you can easily make your own ordination matrices by using the following functions from the stats package. Here is how to do it for the following types of matrices: Multivariate matrices (principal components analysis; PCA) ## A multivariate matrix head(USArrests) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 ## Ordinating the matrix using `prcomp` ordination &lt;- prcomp(USArrests) ## Selecting the ordinated matrix ordinated_matrix &lt;- ordination$x head(ordinated_matrix) ## PC1 PC2 PC3 PC4 ## Alabama 64.80216 -11.448007 -2.4949328 -2.4079009 ## Alaska 92.82745 -17.982943 20.1265749 4.0940470 ## Arizona 124.06822 8.830403 -1.6874484 4.3536852 ## Arkansas 18.34004 -16.703911 0.2101894 0.5209936 ## California 107.42295 22.520070 6.7458730 2.8118259 ## Colorado 34.97599 13.719584 12.2793628 1.7214637 This results in a ordinated matrix with US states as elements and four dimensions (PC 1 to 4). For an alternative method, see the ?princomp function. Distance matrices (classical multidimensional scaling; MDS) ## A matrix of distances between cities str(eurodist) ## Class &#39;dist&#39; atomic [1:210] 3313 2963 3175 3339 2762 ... ## ..- attr(*, &quot;Size&quot;)= num 21 ## ..- attr(*, &quot;Labels&quot;)= chr [1:21] &quot;Athens&quot; &quot;Barcelona&quot; &quot;Brussels&quot; &quot;Calais&quot; ... ## Ordinating the matrix using cmdscale() with k = 5 dimensions ordinated_matrix &lt;- cmdscale(eurodist, k = 5) head(ordinated_matrix) ## [,1] [,2] [,3] [,4] [,5] ## Athens 2290.27468 1798.8029 53.79314 -103.82696 -156.95511 ## Barcelona -825.38279 546.8115 -113.85842 84.58583 291.44076 ## Brussels 59.18334 -367.0814 177.55291 38.79751 -95.62045 ## Calais -82.84597 -429.9147 300.19274 106.35369 -180.44614 ## Cherbourg -352.49943 -290.9084 457.35294 111.44915 -417.49668 ## Cologne 293.68963 -405.3119 360.09323 -636.20238 159.39266 This results in a ordinated matrix with European cities as elements and five dimensions. Of course any other method for creating the ordination matrix is totally valid, you can also not use any ordination at all! The only requirements for the dispRity functions is that the input is a matrix with elements as rows and dimensions as columns. 3.2 Performing a simple dispRity analysis Two dispRity functions allow users to run an analysis pipeline simply by inputting an ordination matrix. These functions allow users to either calculate the disparity through time (dispRity.through.time) or the disparity of user-defined groups (dispRity.per.group). IMPORTANT Note that disparity.through.time and disparity.per.group are wrapper functions (i.e. they incorporate lots of other functions) that allow users to run a basic disparity-through-time, or disparity among groups, analysis without too much effort. As such they have a lot of defaults. These are described in the help files for the functions that are used to make the wrapper functions, and not described in the help files for disparity.through.time and disparity.per.group. These defaults are perfect for data exploration, but for a proper analysis you should consider the best parameters for your question and data. For example, which metric should you use? How many bootstraps do you require? What model of evolution is most appropriate if you are time slicing? Should you rarefy the data? See custom.subsamples, boot.matrix, dispRity.metric, , plot.dispRity for more details of the defaults used in each of these functions. Note that any of these defaults can be changed within the disparity.through.time or disparity.per.group functions. 3.2.1 Example data To illustrate these functions, we will use data from @beckancient2014. This dataset contains an ordinated matrix of 50 discrete characters from mammals (BeckLee_mat50), another matrix of the same 50 mammals and the estimated discrete data characters of their descendants (thus 50 + 49 rows, BeckLee_mat99), a dataframe containing the ages of each taxon in the dataset (BeckLee_ages) and finally a phylogenetic tree with the relationships among the 50 mammals (BeckLee_tree). ## Loading the ordinated matrices data(BeckLee_mat50) data(BeckLee_mat99) ## The first five taxa and dimensions of the 50 taxa matrix head(BeckLee_mat50[, 1:5]) ## [,1] [,2] [,3] [,4] [,5] ## Cimolestes -0.5319679 0.1117759259 0.09865194 -0.1933148 0.2035833 ## Maelestes -0.4087147 0.0139690317 0.26268300 0.2297096 0.1310953 ## Batodon -0.6923194 0.3308625215 -0.10175223 -0.1899656 0.1003108 ## Bulaklestes -0.6802291 -0.0134872777 0.11018009 -0.4103588 0.4326298 ## Daulestes -0.7386111 0.0009001369 0.12006449 -0.4978191 0.4741342 ## Uchkudukodon -0.5105254 -0.2420633915 0.44170317 -0.1172972 0.3602273 ## The first five taxa and dimensions of the 99 taxa + ancestors matrix BeckLee_mat99[c(1, 2, 98, 99), 1:5] ## [,1] [,2] [,3] [,4] [,5] ## Cimolestes -0.60824375 -0.0323683 0.08458885 -0.43384481 -0.30536875 ## Maelestes -0.57302058 -0.2840361 0.01308847 -0.12588477 0.06123611 ## n48 -0.05529018 0.4799330 0.04118477 0.04944912 -0.35588301 ## n49 -0.13067785 0.4478168 0.11956268 0.13800340 -0.32227852 ## Loading a list of first and last occurrence dates for the fossils data(BeckLee_ages) head(BeckLee_ages) ## FAD LAD ## Adapis 37.2 36.8 ## Asioryctes 83.6 72.1 ## Leptictis 33.9 33.3 ## Miacis 49.0 46.7 ## Mimotona 61.6 59.2 ## Notharctus 50.2 47.0 ## Loading and plotting the phylogeny data(BeckLee_tree) plot(BeckLee_tree, cex = 0.8) axisPhylo(root = 140) nodelabels(cex = 0.5) Of course you can use your own data as detailed in the previous chapter. 3.2.2 Disparity through time The dispRity.through.time function calculates disparity through time, a common analysis in palaeontology. This function (and the following one) uses an analysis pipeline with a lot of default parameters to make the analysis as simple as possible. Of course all the defaults can be changed if required, more on this later. For a disparity through time analysis, you will need: An ordinated matrix (we covered that above) A phylogenetic tree: this must be a phylo object (from the ape package) and needs a root.time element. To give your tree a root time (i.e. an age for the root), you can simply do my_tree$root.time &lt;- my_age. The required number of time subsamples (here time = 3) Your favourite disparity metric (here the sum of variances) Using the @beckancient2014 data described above: ## Measuring disparity through time disparity_data &lt;- dispRity.through.time(BeckLee_mat50, BeckLee_tree, time = 3, metric = c(sum, variances)) This generates a dispRity object (see here for technical details). When displayed, these dispRity objects provide us with information on the operations done to the matrix: ## Print the disparity_data object disparity_data ## ---- dispRity object ---- ## 3 discrete time subsamples for 50 elements with 48 dimensions: ## 133.51104 - 89.00736, 89.00736 - 44.50368, 44.50368 - 0. ## Data was bootstrapped 100 times (method:&quot;full&quot;). ## Disparity was calculated as: metric. We asked for three subsamples (evenly spread across the age of the tree), the data was bootstrapped 100 times (default) and the metric used was the sum of variances. We can now summarise or plot the disparity_data object, or perform statistical tests on it (e.g. a simple lm): ## Summarising disparity through time summary(disparity_data) ## subsamples n obs bs.median 2.5% 25% 75% 97.5% ## 1 133.51104 - 89.00736 5 1.575 1.305 0.729 1.118 1.420 1.509 ## 2 89.00736 - 44.50368 29 1.922 1.867 1.775 1.830 1.889 1.922 ## 3 44.50368 - 0 16 1.990 1.871 1.716 1.831 1.914 1.942 ## Plotting the results plot(disparity_data, type = &quot;continuous&quot;) ## Testing for an difference among the time bins disp_lm &lt;- test.dispRity(disparity_data, test = lm, comparisons = &quot;all&quot;) summary(disp_lm) ## ## Call: ## test(formula = data ~ subsamples, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.56623 -0.04160 0.01049 0.05507 0.31886 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.25647 0.01270 98.97 &lt;2e-16 *** ## subsamples44.50368 - 0 0.60863 0.01795 33.90 &lt;2e-16 *** ## subsamples89.00736 - 44.50368 0.60169 0.01795 33.51 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.127 on 297 degrees of freedom ## Multiple R-squared: 0.8361, Adjusted R-squared: 0.835 ## F-statistic: 757.5 on 2 and 297 DF, p-value: &lt; 2.2e-16 ## Note that ANOVA assumptions were not checked here! Please refer to the specific tutorials for (much!) more information on the nuts and bolts of the package. You can also directly explore the specific function help files within R and navigate to related functions. 3.2.3 Disparity among groups The dispRity.per.group function is used if you are interested in looking at disparity among groups rather than through time. For example, you could ask if there is a difference in disparity between two groups? To perform such an analysis, you will need: An matrix with rows as elements and columns as dimensions (always!) A list of group members: this list should be a list of numeric vectors or names corresponding to the row names in the matrix. For example list(&quot;a&quot; = c(1,2), &quot;b&quot; = c(3,4)) will create a group a containing elements 1 and 2 from the matrix and a group b containing elements 3 and 4. Note that elements can be present in multiple groups at once. Your favourite disparity metric (here the sum of variances) Using the @beckancient2014 data described above: ## Creating the two groups (crown versus stem) as a list mammal_groups &lt;- list(&quot;crown&quot; = c(16, 19:41, 45:50), &quot;stem&quot; = c(1:15, 17:18, 42:44)) ## Measuring disparity for each group disparity_data &lt;- dispRity.per.group(BeckLee_mat50, group = mammal_groups, metric = c(sum, variances)) We can display the disparity of both groups by simply looking at the output variable (disparity_data) and then summarising the disparity_data object and plotting it, and/or by performing a statistical test to compare disparity across the groups (here a Wilcoxon test). ## Print the disparity_data object disparity_data ## ---- dispRity object ---- ## 2 customised subsamples for 50 elements with 48 dimensions: ## crown, stem. ## Data was bootstrapped 100 times (method:&quot;full&quot;). ## Disparity was calculated as: metric. ## Summarising disparity in the different groups summary(disparity_data) ## subsamples n obs bs.median 2.5% 25% 75% 97.5% ## 1 crown 30 1.995 1.936 1.866 1.914 1.947 1.971 ## 2 stem 20 1.715 1.632 1.541 1.604 1.667 1.695 ## Plotting the results plot(disparity_data) ## Testing for a difference between the groups test.dispRity(disparity_data, test = wilcox.test, details = TRUE) ## $`crown : stem` ## $`crown : stem`[[1]] ## ## Wilcoxon rank sum test with continuity correction ## ## data: dots[[1L]][[1L]] and dots[[2L]][[1L]] ## W = 10000, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 "],
["details-of-specific-functions.html", "4 Details of specific functions 4.1 Time slicing 4.2 Customised subsamples 4.3 Bootstraps and rarefactions 4.4 Disparity metrics 4.5 Summarising dispRity data (plots) 4.6 Testing disparity hypotheses 4.7 Disparity as a distribution 4.8 Ecological studies of disparity 4.9 Simulating discrete morphological data", " 4 Details of specific functions The following section contains information specific to some functions. If any of your questions are not covered in these sections, please refer to the function help files in R, send me an email (guillert@tcd.ie), or raise an issue on GitHub. The several tutorials below describe specific functionalities of certain functions; please always refer to the function help files for the full function documentation! 4.1 Time slicing The function time.subsamples allows users to divide the matrix into different time subsamples or slices given a dated phylogeny that contains all the elements (i.e. taxa) from the matrix. Each subsample generated by this function will then contain all the elements present at a specific point in time or during a specific period in time. Two types of time subsamples can be performed by using the method option: Discrete time subsamples (or time-binning) using method = discrete Continuous time subsamples (or time-slicing) using method = continuous For the time-slicing method details see @GuillermeSTD. For both methods, the function takes the time argument which can be a vector of numeric values for: Defining the boundaries of the time bins (when method = discrete) Defining the time slices (when method = continuous) Otherwise, the time argument can be set as a single numeric value for automatically generating a given number of equidistant time-bins/slices. Additionally, it is also possible to input a dataframe containing the first and last occurrence data (FAD/LAD) for taxa that span over a longer time than the given tips/nodes age, so taxa can appear in more than one time bin/slice. Here is an example for method = discrete: ## Generating three time bins containing the taxa present every 40 Ma time.subsamples(data = BeckLee_mat50, tree = BeckLee_tree, method = &quot;discrete&quot;, time = c(120, 80, 40, 0)) ## ---- dispRity object ---- ## 3 discrete time subsamples for 50 elements: ## 120 - 80, 80 - 40, 40 - 0. Note that we can also generate equivalent results by just telling the function that we want three time-bins as follow: ## Automatically generate three equal length bins: time.subsamples(data = BeckLee_mat50, tree = BeckLee_tree, method = &quot;discrete&quot;, time = 3) ## ---- dispRity object ---- ## 3 discrete time subsamples for 50 elements: ## 133.51104 - 89.00736, 89.00736 - 44.50368, 44.50368 - 0. In this example, the taxa were split inside each time-bin according to their age. However, the taxa here are considered as single points in time. It is totally possible that some taxa could have had longer longevity and that they exist in multiple time bins. In this case, it is possible to include them in more than one bin by providing a table of first and last occurrence dates (FAD/LAD). This table should have the taxa names as row names and two columns for respectively the first and last occurrence age: ## Displaying the table of first and last occurrence dates for each taxa head(BeckLee_ages) ## FAD LAD ## Adapis 37.2 36.8 ## Asioryctes 83.6 72.1 ## Leptictis 33.9 33.3 ## Miacis 49.0 46.7 ## Mimotona 61.6 59.2 ## Notharctus 50.2 47.0 ## Generating time bins including taxa that might span between them time.subsamples(data = BeckLee_mat50, tree = BeckLee_tree, method = &quot;discrete&quot;, time = c(120, 80, 40, 0), FADLAD = BeckLee_ages) ## ---- dispRity object ---- ## 3 discrete time subsamples for 50 elements: ## 120 - 80, 80 - 40, 40 - 0. When using this method, the oldest boundary of the first bin (or the first slice, see below) is automatically generated as the root age plus 1% of the tree length, as long as at least three elements/taxa are present at that point in time. The algorithm adds an extra 1% tree length until reaching the required minimum of three elements. It is also possible to include nodes in each bin by using inc.nodes = TRUE and providing a matrix that contains the ordinated distance among tips and nodes. For the time-slicing method (method = continuous), the idea is fairly similar. This option, however, requires a matrix that contains the ordinated distance among taxa and nodes and an extra argument describing the assumed evolutionary model (via the model argument). This model argument is used when the time slice occurs along a branch of the tree rather than on a tip or a node, meaning that a decision must be made about what the value for the branch should be. The model can be one of the following: acctran where the data chosen along the branch is always the one of the descendant deltran where the data chosen along the branch is always the one of the ancestor punctuated where the data chosen along the branch is randomly chosen between the descendant or the ancestor gradual where the data chosen along the branch is either the descendant or the ancestor depending on branch length ## Generating four time slices every 40 million years under a model of gradual evolution time.subsamples(data = BeckLee_mat99, tree = BeckLee_tree, method = &quot;continuous&quot;, model = &quot;gradual&quot;, time = c(120, 80, 40, 0), FADLAD = BeckLee_ages) ## ---- dispRity object ---- ## 4 continuous (gradual) time subsamples for 99 elements: ## 120, 80, 40, 0. ## Generating four time slices automatically time.subsamples(data = BeckLee_mat99, tree = BeckLee_tree, method = &quot;continuous&quot;, model = &quot;gradual&quot;, time = 4, FADLAD = BeckLee_ages) ## ---- dispRity object ---- ## 4 continuous (gradual) time subsamples for 99 elements: ## 133.51104, 89.00736, 44.50368, 0. 4.2 Customised subsamples Another way of separating elements into different categories is to use customised subsamples as briefly explained above. This function simply takes the list of elements to put in each group (whether they are the actual element names or their position in the matrix). ## Creating the two groups as a list mammal_groups &lt;- list(&quot;crown&quot; = c(16, 19:41, 45:50), &quot;stem&quot; = c(1:15, 17:18, 42:44)) ## Separating the dataset into two different groups custom.subsamples(BeckLee_mat50, group = mammal_groups) ## ---- dispRity object ---- ## 2 customised subsamples for 50 elements: ## crown, stem. Elements can easily be assigned to different groups if necessary! ## Creating the three groups as a list mammal_groups &lt;- list(&quot;crown&quot; = c(16, 19:41, 45:50), &quot;stem&quot; = c(1:15, 17:18, 42:44). &quot;all&quot; = c(1:50)) 4.3 Bootstraps and rarefactions One important step in analysing ordinated matrices is to pseudo-replicate the data to see how robust the results are, and how sensitive they are to outliers in the dataset. This can be achieved using the function boot.matrix to bootstrap and/or rarefy the data. The default options will bootstrap the matrix 100 times without rarefaction using the “full” bootstrap method (see below): ## Default bootstrapping boot.matrix(data = BeckLee_mat50) ## ---- dispRity object ---- ## 50 elements with 48 dimensions. ## Data was bootstrapped 100 times (method:&quot;full&quot;). The number of bootstrap replicates can be defined using the bootstraps option. The method can be modified by controlling which bootstrap algorithm to use through the boot.type argument. Currently two algorithms are implemented: full where the bootstrapping is entirely stochastic (n elements are replaced by any m elements drawn from the data) single where only one random element is replaced by one other random element for each pseudo-replicate ## Bootstrapping with the single bootstrap method boot.matrix(BeckLee_mat50, boot.type = &quot;single&quot;) ## ---- dispRity object ---- ## 50 elements with 48 dimensions. ## Data was bootstrapped 100 times (method:&quot;single&quot;). This function also allows users to rarefy the data using the rarefaction argument. Rarefaction allows users to limit the number of elements to be drawn at each bootstrap replication. This is useful if, for example, one is interested in looking at the effect of reducing the number of elements on the results of an analysis. This can be achieved by using the rarefaction option that draws only n-x at each bootstrap replicate (where x is the number of elements not sampled). The default argument is FALSE but it can be set to TRUE to fully rarefy the data (i.e. remove x elements for the number of pseudo-replicates, where x varies from the maximum number of elements present in each subsample to a minimum of three elements). It can also be set to one or more numeric values to only rarefy to the corresponding number of elements. ## Bootstrapping with the full rarefaction boot.matrix(BeckLee_mat50, bootstraps = 20, rarefaction = TRUE) ## ---- dispRity object ---- ## 50 elements with 48 dimensions. ## Data was bootstrapped 20 times (method:&quot;full&quot;) and fully rarefied. ## Or with a set number of rarefaction levels boot.matrix(BeckLee_mat50, bootstraps = 20, rarefaction = c(6:8, 3)) ## ---- dispRity object ---- ## 50 elements with 48 dimensions. ## Data was bootstrapped 20 times (method:&quot;full&quot;) and rarefied to 6, 7, 8, 3 elements. One additional important argument is dimensions that specifies how many dimensions from the matrix should be used for further analysis. When missing, all dimensions from the ordinated matrix are used. ## Using the first 50% of the dimensions boot.matrix(BeckLee_mat50, dimensions = 0.5) ## ---- dispRity object ---- ## 50 elements with 24 dimensions. ## Data was bootstrapped 100 times (method:&quot;full&quot;). ## Using the first 10 dimensions boot.matrix(BeckLee_mat50, dimensions = 10) ## ---- dispRity object ---- ## 50 elements with 10 dimensions. ## Data was bootstrapped 100 times (method:&quot;full&quot;). Of course, one could directly supply the subsamples generated above (using time.subsamples or custom.subsamples) to this function. ## Creating subsamples of crown and stem mammals crown_stem &lt;- custom.subsamples(BeckLee_mat50, group = list(&quot;crown&quot; = c(16, 19:41, 45:50), &quot;stem&quot; = c(1:15, 17:18, 42:44))) ## Bootstrapping and rarefying these groups boot.matrix(crown_stem, bootstraps = 200, rarefaction = TRUE) ## ---- dispRity object ---- ## 2 customised subsamples for 50 elements with 48 dimensions: ## crown, stem. ## Data was bootstrapped 200 times (method:&quot;full&quot;) and fully rarefied. ## Creating time slice subsamples time_slices &lt;- time.subsamples(data = BeckLee_mat99, tree = BeckLee_tree, method = &quot;continuous&quot;, model = &quot;gradual&quot;, time = c(120, 80, 40, 0), FADLAD = BeckLee_ages) ## Bootstrapping the time slice subsamples boot.matrix(time_slices, bootstraps = 100) ## ---- dispRity object ---- ## 4 continuous (gradual) time subsamples for 99 elements with 97 dimensions: ## 120, 80, 40, 0. ## Data was bootstrapped 100 times (method:&quot;full&quot;). 4.4 Disparity metrics There are many ways of measuring disparity! In brief, disparity is a summary metric that will represent an aspect of an ordinated space (e.g. a MDS, PCA, PCO, PCoA). For example, one can look at ellipsoid hyper-volume of the ordinated space @DonohueDim, the sum and the product of the ranges and variances @Wills1994 or the median position of the elements relative to their centroid @Wills1994. Of course, there are many more examples of metrics one can use for describing some aspect of the ordinated space, with some performing better than other ones at particular descriptive tasks, and some being more generalist. Because of this great diversity of metrics, the package dispRity does not have one way to measure disparity but rather proposes to facilitate users in defining their own disparity metric that will best suit their particular analysis. In fact, the core function of the package, dispRity, allows the user to define any metric with the metric argument. However the metric argument has to follow certain rules: It must be composed from one to three function objects; The function(s) must take as a first argument a matrix or a vector; The function(s) must be of one of the three dimension-levels described below; At least one of the functions must be of dimension-level 1 or 2 (see below). 4.4.1 The function dimension-levels The metric function dimension-levels determine the “dimensionality of decomposition” of the input matrix. In other words, each dimension-level designates the dimensions of the output, i.e. either three (a matrix); two (a vector); or one (a single numeric value) dimension. Illustration of the different dimension-levels of functions with an input matrix 4.4.1.1 Dimension-level 1 functions A dimension-level 1 function will decompose a matrix or a vector into a single value: ## Creating a dummy matrix dummy_matrix &lt;- matrix(rnorm(12), 4, 3) ## Example of dimension-level 1 functions mean(dummy_matrix) ## [1] -0.3227241 median(dummy_matrix) ## [1] -0.2690165 Any summary metric such as mean or median are good examples of dimension-level 1 functions as they reduce the matrix to a single dimension (i.e. one value). 4.4.1.2 Dimension-level 2 functions A dimension-level 2 function will decompose a matrix into a vector. ## Defining the function as the product of rows prod.rows &lt;- function(matrix) apply(matrix, 1, prod) ## A dimension-level 2 metric prod.rows(dummy_matrix) ## [1] -1.2630799 0.2148864 -0.1797556 -0.2421790 Several dimension-level 2 functions are implemented in dispRity (see ?dispRity.metric) such as the variances or ranges functions that calculate the variance or the range of each dimension of the ordinated matrix respectively. 4.4.1.3 Dimension-level 3 functions Finally a dimension-level 3 function will transform the matrix into another matrix. Note that the dimension of the output matrix doesn’t need to match the the input matrix: ## A dimension-level 3 metric var(dummy_matrix) ## [,1] [,2] [,3] ## [1,] 1.916420 1.9501955 0.2907060 ## [2,] 1.950196 3.2948635 0.3958234 ## [3,] 0.290706 0.3958234 0.6704976 ## A dimension-level 3 metric with a forced matrix output as.matrix(dist(dummy_matrix)) ## 1 2 3 4 ## 1 0.000000 5.140790 4.005068 3.120827 ## 2 5.140790 0.000000 2.179318 2.982267 ## 3 4.005068 2.179318 0.000000 2.174896 ## 4 3.120827 2.982267 2.174896 0.000000 4.4.2 make.metric Of course, functions can be more complex and involve multiple operations such as the centroids function (see ?dispRity.metric) that calculates the Euclidean distance between each element and the centroid of the ordinated space. The make.metric function implemented in dispRity is designed to help test and find the dimension-level of the functions. This function tests: If your function can deal with a matrix or a vector as an input; Your function’s dimension-level according to its output (dimension-level 1, 2 or 3, see above); Whether the function can be implemented in the dispRity function (the function is fed into a lapply loop). For example, let’s see if the functions described above are the right dimension-levels: ## Which dimension-level is the mean function? And can it be used in dispRity? make.metric(mean) ## mean outputs a single value. ## mean is detected as being a dimension-level 1 function. ## Which dimension-level is the prod.rows function? And can it be used in dispRity? make.metric(prod.rows) ## prod.rows outputs a matrix object. ## prod.rows is detected as being a dimension-level 2 function. ## Which dimension-level is the var function? And can it be used in dispRity? make.metric(var) ## var outputs a matrix object. ## var is detected as being a dimension-level 3 function. ## Additional dimension-level 2 and/or 1 function(s) will be needed. A non verbose version of the function is also available. This can be done using the option silent = TRUE and will simply output the dimension-level of the metric. ## Testing whether mean is dimension-level 1 if(make.metric(mean, silent = TRUE) != &quot;level1&quot;) { message(&quot;The metric is not dimension-level 1.&quot;) } ## Testing whether var is dimension-level 1 if(make.metric(var, silent = TRUE) != &quot;level1&quot;) { message(&quot;The metric is not dimension-level 1.&quot;) } ## The metric is not dimension-level 1. 4.4.3 Metrics in the dispRity function Using this metric structure, we can easily use any disparity metric in the dispRity function as follows: ## Measuring disparity as the standard deviation of all the values of the ## ordinated matrix (dimension-level 1 function). summary(dispRity(BeckLee_mat50, metric = sd)) ## subsamples n obs ## 1 1 50 0.201 ## Measuring disparity as the standard deviation of the variance of each axis of ## the ordinated matrix (dimension-level 1 and 2 functions). summary(dispRity(BeckLee_mat50, metric = c(sd, variances))) ## subsamples n obs ## 1 1 50 0.028 ## Measuring disparity as the standard deviation of the variance of each axis of ## the variance covariance matrix (dimension-level 1, 2 and 3 functions). summary(dispRity(BeckLee_mat50, metric = c(sd, variances, var)), round = 10) ## subsamples n obs ## 1 1 50 0.0001025857 Note that the order of each function in the metric argument does not matter, the dispRity function will automatically detect the function dimension-levels (using make.metric) and apply them to the data in decreasing order (dimension-level 3 &gt; 2 &gt; 1). ## Disparity as the standard deviation of the variance of each axis of the ## variance covariance matrix: disparity1 &lt;- summary(dispRity(BeckLee_mat50, metric = c(sd, variances, var)), round = 10) ## Same as above but using a different function order for the metric argument disparity2 &lt;- summary(dispRity(BeckLee_mat50, metric = c(variances, sd, var)), round = 10) ## Both ways output the same disparity values: disparity1 == disparity2 ## subsamples n obs ## [1,] TRUE TRUE TRUE In these examples, we considered disparity to be a single value. For example, in the previous example, we defined disparity as the standard deviation of the variances of each column of the variance/covariance matrix (metric = c(variances, sd, var)). It is, however, possible to calculate disparity as a distribution. 4.4.4 Metrics implemented in dispRity Several disparity metrics are implemented in the dispRity package. The detailed list can be found in ?dispRity.metric along with some description of each metric. Level Name Description Source 3 ellipse.volume1 The volume of the ellipsoid of the space [@DonohueDim] 3 convhull.surface The surface of the convex hull formed by all the elements geometry::convhulln 3 convhull.volume The volume of the convex hull formed by all the elements geometry::convhulln 3 hypervolume The volume of the ordinated space hypervolume::hypervolume 3 diagonal The longest distance in the ordinated space (like the diagonal in two dimensions) dispRity 2 ranges The range of each dimension dispRity 2 variances The variance of each dimension dispRity 2 centroids2 The distance between each element and the centroid of the ordinated space dispRity 1 mode.val The modal value dispRity 1: This function uses an estimation of the eigenvalue that only works for MDS or PCoA ordinations (not PCA). 2: Note that by default, the centroid is the centroid of the elements. It can, however, be fixed to a different value by using the centroid argument centroids(space, centroid = rep(0, ncol(space))), for example the origin of the ordinated space. 4.4.5 Equations and implementations Some of the functions described below are implemented in the dispRity package and do not require any other packages to calculate (see implementation here). \\[\\begin{equation} ellipse.volume = \\frac{\\pi^{k/2}}{\\Gamma(\\frac{k}{2}+1)}\\displaystyle\\prod_{i=1}^{k} (\\lambda_{i}^{0.5}) \\end{equation}\\] Where k is the number of dimensions, and \\(\\lambda_i\\) is the eigenvalue of each dimension. \\[\\begin{equation} diagonal = \\sqrt{\\sum_{i=1}^{k}|max(k_i) - min(k_i)|} \\end{equation}\\] Where k is the number of dimensions. \\[\\begin{equation} ranges = |max(k_i) - min(k_i)| \\end{equation}\\] Where k is the number of dimensions. \\[\\begin{equation} variances = \\sigma^{2}{k_i} \\end{equation}\\] Where k is the number of dimensions, and \\(\\sigma^{2}\\) is their variance. \\[\\begin{equation} centroids = \\sqrt{\\sum_{i=1}^{n}{({k}_{n}-Centroid_{k})^2}} \\end{equation}\\] Where n is each element in the ordinated space, k is the number of dimensions, and \\(Centroid_{k}\\) is their mean (or can be set to another value). 4.4.6 Using the different disparity metrics Here is a brief demonstration of the main metrics implemented in dispRity. First, we will create a dummy/simulated ordinated space using the space.maker utility function (more about that here: ## Creating a 10*5 normal space set.seed(1) dummy_space &lt;- space.maker(10, 5, rnorm) We will use this simulated space to demonstrate the different metrics. 4.4.6.1 Volumes and surface metrics The functions ellipse.volume, convhull.surface, convhull.volume and hyper.volume all measure the surface or the volume of the ordinated space occupied: ## Calculating the ellipsoid volume summary(dispRity(dummy_space, metric = ellipse.volume)) ## subsamples n obs ## 1 1 10 257.8 Because there is only one subsample (i.e. one matrix) in the dispRity object, this operation is the equivalent of ellipse.volume(dummy_space) (with rounding). ## Calculating the convex hull surface summary(dispRity(dummy_space, metric = convhull.surface)) ## subsamples n obs ## 1 1 10 11.91 ## Calculating the convex hull volume summary(dispRity(dummy_space, metric = convhull.volume)) ## subsamples n obs ## 1 1 10 1.031 The convex hull functions make a (good) estimation of the multidimensional properties of the ordinated space. For the full and correct calculation of the volume of the ordinated space, it is possible to use the hyper.volume function that has more options but takes longer to calculate. ## Calculating the true multidimensional volume summary(dispRity(dummy_space, metric = hyper.volume)) ## Warning in hypervolume::hypervolume(matrix, method = method, ...): Log number of observations (1.61) is less than or equal to the number of dimensions (4). ## You may not have enough data to accurately estimate a hypervolume with this dimensionality. ## Consider reducing the dimensionality of the analysis. ## Warning in hypervolume::hypervolume(matrix, method = method, ...): Log number of observations (2.30) is less than or equal to the number of dimensions (5). ## You may not have enough data to accurately estimate a hypervolume with this dimensionality. ## Consider reducing the dimensionality of the analysis. ## subsamples n obs ## 1 1 10 408.2 Cautionary note: measuring volumes in a high number of dimensions can be strongly affected by the curse of dimensionality that often results in near 0 disparity values. 4.4.6.2 Ranges, variances and diagonal The functions ranges, variances and diagonal all measure properties of the ordinated space based on its dimensional properties (they are also less affected by the “curse of dimensionality”): ranges and variances both work on the same principle and measure the range/variance of each dimension: ## Calculating the ranges of each dimension in the ordinated space ranges(dummy_space) ## [1] 2.430909 3.726481 2.908329 2.735739 1.588603 ## Calculating disparity as the distribution of these ranges summary(dispRity(dummy_space, metric = ranges)) ## subsamples n obs.median 2.5% 25% 75% 97.5% ## 1 1 10 2.736 1.673 2.431 2.908 3.645 ## Calculating disparity as the sum and the product of these ranges summary(dispRity(dummy_space, metric = c(sum, ranges))) ## subsamples n obs ## 1 1 10 13.39 summary(dispRity(dummy_space, metric = c(prod, ranges))) ## subsamples n obs ## 1 1 10 114.5 ## Calculating the variances of each dimension in the ordinated space variances(dummy_space) ## [1] 0.6093144 1.1438620 0.9131859 0.6537768 0.3549372 ## Calculating disparity as the distribution of these variances summary(dispRity(dummy_space, metric = variances)) ## subsamples n obs.median 2.5% 25% 75% 97.5% ## 1 1 10 0.654 0.38 0.609 0.913 1.121 ## Calculating disparity as the sum and the product of these variances summary(dispRity(dummy_space, metric = c(sum, variances))) ## subsamples n obs ## 1 1 10 3.675 summary(dispRity(dummy_space, metric = c(prod, variances))) ## subsamples n obs ## 1 1 10 0.148 The diagonal function measures the multidimensional diagonal of the whole space (i.e. in our case the longest Euclidean distance in our five dimensional space): ## Calculating the ordinated space&#39;s diagonal summary(dispRity(dummy_space, metric = diagonal)) ## subsamples n obs ## 1 1 10 3.659 This metric is only a Euclidean diagonal (mathematically valid) if the dimensions within the space are all orthogonal! 4.4.6.3 Centroids metric The centroids metric allows users to measure the position of the different elements compared to a fixed point in the ordinated space. By default, this function measures the distance between each element and their centroid (centre point): ## The distribution of the distances between each element and their centroid summary(dispRity(dummy_space, metric = centroids)) ## subsamples n obs.median 2.5% 25% 75% 97.5% ## 1 1 10 1.435 0.788 1.267 1.993 3.167 ## Disparity as the median value of these distances summary(dispRity(dummy_space, metric = c(median, centroids))) ## subsamples n obs ## 1 1 10 1.435 It is however possible to fix the coordinates of the centroid to a specific point in the ordinated space, as long as it has the correct number of dimensions: ## The distance between each element and the origin of the ordinated space summary(dispRity(dummy_space, metric = centroids, centroid = c(0,0,0,0,0))) ## subsamples n obs.median 2.5% 25% 75% 97.5% ## 1 1 10 1.487 0.785 1.2 2.044 3.176 ## Disparity as the distance between each element and a specific point in space summary(dispRity(dummy_space, metric = centroids, centroid = c(0,1,2,3,4))) ## subsamples n obs.median 2.5% 25% 75% 97.5% ## 1 1 10 5.489 4.293 5.032 6.155 6.957 4.5 Summarising dispRity data (plots) Because of its architecture, printing dispRity objects only summarises their content but does not print the disparity value measured or associated analysis (more about this here). To actually see what is in a dispRity object, one can either use the summary function for visualising the data in a table or plot to have a graphical representation of the results. 4.5.1 Summarising dispRity data This function is an S3 function (summary.dispRity) allowing users to summarise the content of dispRity objects that contain disparity calculations. ## Example data from previous sections crown_stem &lt;- custom.subsamples(BeckLee_mat50, group = list(&quot;crown&quot; = c(16, 19:41, 45:50), &quot;stem&quot; = c(1:15, 17:18, 42:44))) ## Bootstrapping and rarefying these groups boot_crown_stem &lt;- boot.matrix(crown_stem, bootstraps = 100, rarefaction = TRUE) ## Calculate disparity disparity_crown_stem &lt;- dispRity(boot_crown_stem, metric = c(sum, variances)) ## Creating time slice subsamples time_slices &lt;- time.subsamples(data = BeckLee_mat99, tree = BeckLee_tree, method = &quot;continuous&quot;, model = &quot;gradual&quot;, time = c(120, 80, 40, 0), FADLAD = BeckLee_ages) ## Bootstrapping the time slice subsamples boot_time_slices &lt;- boot.matrix(time_slices, bootstraps = 100) ## Calculate disparity disparity_time_slices &lt;- dispRity(boot_time_slices, metric = c(sum, variances)) ## Creating time bin subsamples time_bins &lt;- time.subsamples(data = BeckLee_mat99, tree = BeckLee_tree, method = &quot;discrete&quot;, time = c(120, 80, 40, 0), FADLAD = BeckLee_ages, inc.nodes = TRUE) ## Bootstrapping the time bin subsamples boot_time_bins &lt;- boot.matrix(time_bins, bootstraps = 100) ## Calculate disparity disparity_time_bins &lt;- dispRity(boot_time_bins, metric = c(sum, variances)) These objects are easy to summarise as follows: ## Default summary summary(disparity_time_slices) ## subsamples n obs bs.median 2.5% 25% 75% 97.5% ## 1 120 5 2.823 2.295 1.292 2.039 2.598 2.829 ## 2 80 19 3.233 3.079 2.834 2.996 3.138 3.278 ## 3 40 15 3.421 3.211 2.909 3.103 3.309 3.518 ## 4 0 10 4.055 3.684 3.247 3.557 3.783 3.940 Information about the number of elements in each subsample and the observed (i.e. non-bootstrapped) disparity are also calculated. This is specifically handy when rarefying the data for example: head(summary(disparity_crown_stem)) ## subsamples n obs bs.median 2.5% 25% 75% 97.5% ## 1 crown 30 1.995 1.926 1.863 1.912 1.948 1.965 ## 2 crown 29 NA 1.931 1.878 1.909 1.944 1.967 ## 3 crown 28 NA 1.931 1.882 1.908 1.948 1.978 ## 4 crown 27 NA 1.930 1.859 1.908 1.946 1.966 ## 5 crown 26 NA 1.928 1.867 1.907 1.948 1.978 ## 6 crown 25 NA 1.934 1.853 1.904 1.948 1.974 The summary functions can also take various options such as: quantile values for the confidence interval levels (by default, the 50 and 95 quantiles are calculated) cent.tend for the central tendency to use for summarising the results (default is median) roundingoption corresponding to the number of decimal places to print (default is2`) recall option for printing the call of the dispRity object as well (default is FALSE) These options can easily be changed from the defaults as follows: ## Same as above but using the 88th quantile and the standard deviation as the summary summary(disparity_time_slices, quantile = 88, cent.tend = sd) ## subsamples n obs bs.sd 6% 94% ## 1 120 5 2.823 0.398 1.658 2.705 ## 2 80 19 3.233 0.116 2.896 3.243 ## 3 40 15 3.421 0.161 2.967 3.485 ## 4 0 10 4.055 0.184 3.342 3.907 ## Printing the details of the object and rounding the values to the 5th decimal place summary(disparity_time_slices, recall = TRUE, rounding = 5) ## ---- dispRity object ---- ## 4 continuous (gradual) time subsamples for 99 elements with 97 dimensions: ## 120, 80, 40, 0. ## Data was bootstrapped 100 times (method:&quot;full&quot;). ## Disparity was calculated as: c(sum, variances). ## subsamples n obs bs.median 2.5% 25% 75% 97.5% ## 1 120 5 2.82292 2.29498 1.29247 2.03886 2.59781 2.82881 ## 2 80 19 3.23312 3.07949 2.83426 2.99627 3.13758 3.27800 ## 3 40 15 3.42091 3.21071 2.90858 3.10280 3.30911 3.51829 ## 4 0 10 4.05457 3.68433 3.24678 3.55676 3.78275 3.93967 Note that the summary table is a data.frame, hence it is as easy to modify as any dataframe using dplyr. You can also export it in csv format using write.csv or write_csv or even directly export into LaTeX format using the following; ## Loading the xtable package require(xtable) ## Converting the table in LaTeX xtable(summary(disparity_time_slices)) 4.5.2 Plotting dispRity data An alternative (and more fun!) way to display the calculated disparity is to plot the results using the S3 method plot.dispRity. This function takes the same options as summary.dispRity along with various graphical options described in the function help files (see ?plot.dispRity). The plots can be of four different types: continuous for displaying continuous disparity curves box, lines, and polygons to display discrete disparity results in respectively a boxplot, confidence interval lines, and confidence interval polygons. This argument can be left empty. In this case, the algorithm will automatically detect the type of subsamples from the dispRity object and plot accordingly. It is also possible to display the number of elements in each subsample (as a horizontal dotted line) using the option elements = TRUE. Additionally, when the data is rarefied, one can indicate which level of rarefaction to display (i.e. only display the results for a certain number of elements) by using the rarefaction argument. ## Graphical parameters op &lt;- par(mfrow = c(2, 2), bty = &quot;n&quot;) ## Plotting continuous disparity results plot(disparity_time_slices, type = &quot;continuous&quot;) ## Plotting discrete disparity results plot(disparity_crown_stem, type = &quot;box&quot;) ## As above but using lines for the rarefaction level of 20 elements only plot(disparity_crown_stem, type = &quot;line&quot;, rarefaction = 20) ## As above but using polygons while also displaying the number of elements plot(disparity_crown_stem, type = &quot;polygon&quot;, elements = TRUE) ## Resetting graphical parameters par(op) Since plot.dispRity uses the arguments from the generic plot method, it is of course possible to change pretty much everything using the regular plot arguments: ## Graphical options op &lt;- par(bty = &quot;n&quot;) ## Plotting the results with some classic options from plot plot(disparity_time_slices, col = c(&quot;blue&quot;, &quot;orange&quot;, &quot;green&quot;), ylab = c(&quot;Some measurement&quot;), xlab = &quot;Some other measurement&quot;, main = &quot;Many options...&quot;, ylim = c(5, 0), xlim = c(4, 0)) ## Adding a legend legend(&quot;topleft&quot;, legend = c(&quot;Central tendency&quot;, &quot;Confidence interval 1&quot;, &quot;Confidence interval 2&quot;), col = c(&quot;blue&quot;, &quot;orange&quot;, &quot;green&quot;), pch = 19) ## Resetting graphical parameters par(op) In addition to the classic plot arguments, the function can also take arguments that are specific to plot.dispRity like adding the number of elements or rarefaction level (as described above), and also changing the values of the quantiles to plot as well as the central tendency. ## Graphical options op &lt;- par(bty = &quot;n&quot;) ## Plotting the results with some plot.dispRity arguments plot(disparity_time_slices, quantile = c(seq(from = 10, to = 100, by = 10)), cent.tend = sd, type = &quot;c&quot;, elements = TRUE, col = c(&quot;black&quot;, rainbow(10)), ylab = c(&quot;Disparity&quot;, &quot;Diversity&quot;), time.subsamples = FALSE, xlab = &quot;Time (in in units from past to present)&quot;, observed = TRUE, main = &quot;Many more options...&quot;) ## Resetting graphical parameters par(op) Note that the argument observed = TRUE allows to plot the disparity values calculated from the non-bootstrapped data as crosses on the plot. For comparing results, it is also possible to add a plot to the existent plot by using add = TRUE: ## Graphical options op &lt;- par(bty = &quot;n&quot;) ## Plotting the continuous disparity with a fixed y axis plot(disparity_time_slices, ylim = c(1, 4)) ## Adding the discrete data plot(disparity_time_bins, type = &quot;line&quot;, ylim = c(1, 4), xlab = &quot;&quot;, ylab = &quot;&quot;, add = TRUE) ## Resetting graphical parameters par(op) Finally, if your data has been fully rarefied, it is also possible to easily look at rarefaction curves by using the rarefaction = TRUE argument: ## Graphical options op &lt;- par(bty = &quot;n&quot;) ## Plotting the rarefaction curves plot(disparity_crown_stem, rarefaction = TRUE) ## Resetting graphical parameters par(op) 4.6 Testing disparity hypotheses The dispRity package allows users to apply statistical tests to the calculated disparity to test various hypotheses. The function test.dispRity works in a similar way to the dispRity function: it takes a dispRity object, a test and a comparisons argument. The comparisons argument indicates the way the test should be applied to the data: pairwise (default): to compare each subsample in a pairwise manner referential: to compare each subsample to the first subsample sequential: to compare each subsample to the following subsample all: to compare all the subsamples together (like in analysis of variance) It is also possible to input a list of pairs of numeric values or characters matching the subsample names to create personalised tests. Some other tests implemented in dispRity such as the dispRity::null.test have a specific way they are applied to the data and therefore ignore the comparisons argument. The test argument can be any statistical or non-statistical test to apply to the disparity object. It can be a common statistical test function (e.g. stats::t.test), a function implemented in dispRity (e.g. see ?null.test) or any function defined by the user. This function also allows users to correct for Type I error inflation (false positives) when using multiple comparisons via the correction argument. This argument can be empty (no correction applied) or can contain one of the corrections from the stats::p.adjust function (see ?p.adjust). Note that the test.dispRity algorithm deals with some classical test outputs (h.test, lm and numeric vector) and summarises the test output. It is, however, possible to get the full detailed output by using the options details = TRUE. ## T-test to test for a difference in disparity between crown and stem mammals test.dispRity(disparity_crown_stem, test = t.test) ## [[1]] ## statistic ## crown : stem 55.60162 ## ## [[2]] ## parameter ## crown : stem 164.6932 ## ## [[3]] ## p.value ## crown : stem 1.201122e-108 ## Performing the same test but with the detailed t.test output test.dispRity(disparity_crown_stem, test = t.test, details = TRUE) ## $`crown : stem` ## $`crown : stem`[[1]] ## ## Welch Two Sample t-test ## ## data: dots[[1L]][[1L]] and dots[[2L]][[1L]] ## t = 55.602, df = 164.69, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.2849688 0.3059530 ## sample estimates: ## mean of x mean of y ## 1.926163 1.630702 ## Wilcoxon test applied to time sliced disparity with sequential comparisons, with Bonferroni correction test.dispRity(disparity_time_slices, test = wilcox.test, comparisons = &quot;sequential&quot;, correction = &quot;bonferroni&quot;) ## [[1]] ## statistic ## 120 : 80 13 ## 80 : 40 2497 ## 40 : 0 426 ## ## [[2]] ## p.value ## 120 : 80 1.130227e-33 ## 80 : 40 2.904335e-09 ## 40 : 0 1.624453e-28 ## Measuring the overlap between distributions in the time bins (using the ## implemented Bhattacharyya Coefficient function - see ?bhatt.coeff) test.dispRity(disparity_time_bins, test = bhatt.coeff) ## bhatt.coeff ## 120 - 80 : 80 - 40 0.0000000 ## 120 - 80 : 40 - 0 0.0000000 ## 80 - 40 : 40 - 0 0.4195376 It is also possible to apply some more complex tests that have their own output classes (like stats::lm). The results can then be analysed as usual using the associated summary S3 method: ## Performing and linear model applied to the same data (slice_lm &lt;- test.dispRity(disparity_time_slices, test = lm, comparisons = &quot;all&quot;)) ## ## Call: ## test(formula = data ~ subsamples, data = data) ## ## Coefficients: ## (Intercept) subsamples120 subsamples40 subsamples80 ## 3.6571 -1.4046 -0.4481 -0.5863 ## The output is a regular `lm` output class(slice_lm) ## [1] &quot;lm&quot; ## This output can be summarised using summary summary(slice_lm) ## ## Call: ## test(formula = data ~ subsamples, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.26522 -0.10976 0.01515 0.13109 0.58161 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.65712 0.02406 151.99 &lt;2e-16 *** ## subsamples120 -1.40459 0.03403 -41.28 &lt;2e-16 *** ## subsamples40 -0.44812 0.03403 -13.17 &lt;2e-16 *** ## subsamples80 -0.58634 0.03403 -17.23 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2406 on 396 degrees of freedom ## Multiple R-squared: 0.818, Adjusted R-squared: 0.8166 ## F-statistic: 593.1 on 3 and 396 DF, p-value: &lt; 2.2e-16 Of course, due to the modular design of the package, tests can always be made by the user (the same way disparity metrics can be user made). The only condition is that the test can be applied to at least two distributions. In practice, the test.dispRity function will pass the calculated disparity data (distributions) to the provided function in either pairs of distributions (if the comparisons argument is set to pairwise, referential or sequential) or a table containing all the distributions (comparisons = all; this should be in the same format as data passed to lm for example). 4.7 Disparity as a distribution Disparity is often regarded as a summary value of the position of the all elements in the ordinated space. For example, the sum of variances, the product of ranges or the median distance between the elements and their centroid will summarise disparity as a single value. This value can be pseudo-replicated (bootstrapped) to obtain a distribution of the summary metric with estimated error. However, another way to perform disparity analysis is to use the whole distribution rather than just a summary metric (e.g. the variances or the ranges). This is possible in the dispRity package by calculating disparity as a dimension-level 2 metric only! Let’s have a look using our previous example of bootstrapped time slices but by measuring the distances between each taxon and their centroid as disparity. ## Measuring disparity as a whole distribution disparity_centroids &lt;- dispRity(boot_time_slices, metric = centroids) The resulting disparity object is of dimension-level 2, so it can easily be transformed into a dimension-level 1 object by, for example, measuring the median distance of all these distributions: ## Measuring median disparity in each time slice disparity_centroids_median &lt;- dispRity(disparity_centroids, metric = median) And we can now compare the differences between these methods: ## Summarising both disparity measurements: ## The distributions: summary(disparity_centroids) ## subsamples n obs.median bs.median 2.5% 25% 75% 97.5% ## 1 120 5 1.508 1.324 0.536 1.069 1.562 1.868 ## 2 80 19 1.790 1.702 1.393 1.597 1.815 1.943 ## 3 40 15 1.719 1.703 1.387 1.581 1.857 2.076 ## 4 0 10 1.910 1.807 1.366 1.700 1.971 2.085 ## The summary of the distributions (as median) summary(disparity_centroids_median) ## subsamples n obs bs.median 2.5% 25% 75% 97.5% ## 1 120 5 1.508 1.354 0.508 0.915 1.481 1.615 ## 2 80 19 1.790 1.703 1.581 1.664 1.738 1.794 ## 3 40 15 1.719 1.706 1.545 1.646 1.761 1.832 ## 4 0 10 1.910 1.809 1.594 1.784 1.857 1.940 We can see that the summary message for the distribution is slightly different than before. Here summary also displays the observed central tendency (i.e. the central tendency of the measured distributions). Note that, as expected, this central tendency is the same in both metrics! Another, maybe more intuitive way, to compare both approaches for measuring disparity is to plot the distributions: ## Graphical parameters op &lt;- par(bty = &quot;n&quot;, mfrow = c(1, 2)) ## Plotting both disparity measurements plot(disparity_centroids, ylab = &quot;Distribution of all the distances&quot;) plot(disparity_centroids_median, ylab = &quot;Distribution of the medians of all the distances&quot;) par(op) We can then test for differences in the resulting distributions using test.dispRity and the bhatt.coeff test as described above. ## Probability of overlap in the distribution of medians test.dispRity(disparity_centroids_median, test = bhatt.coeff) ## bhatt.coeff ## 120 : 80 0.15380832 ## 120 : 40 0.23690683 ## 120 : 0 0.09196152 ## 80 : 40 0.92367172 ## 80 : 0 0.46576289 ## 40 : 0 0.65992821 In this case, we are looking at the probability of overlap of the distribution of median distances from centroids among each pair of time slices. In other words, we are measuring whether the medians from each bootstrap pseudo-replicate for each time slice overlap. But of course, we might be interested in the actual distribution of the distances from the centroid rather than simply their central tendencies. This can be problematic depending on the research question asked since we are effectively comparing non-independent medians distributions (because of the pseudo-replication). One solution, therefore, is to look at the full distribution: ## Probability of overlap for the full distributions test.dispRity(disparity_centroids, test = bhatt.coeff) ## bhatt.coeff ## 120 : 80 0.6401042 ## 120 : 40 0.6402923 ## 120 : 0 0.5787247 ## 80 : 40 0.9423481 ## 80 : 0 0.8444910 ## 40 : 0 0.9392977 These results show the actual overlap among all the measured distances from centroids concatenated across all the bootstraps. For example, when comparing the slices 120 and 80, we are effectively comparing the 5 100 distances (the distances of the five elements in slice 120 bootstrapped 100 times) to the 19 100 distances from slice 80. However, this can also be problematic for some specific tests since the n 100 distances are also pseudo-replicates and thus are still not independent. A second solution is to compare the distributions to each other for each replicate: ## Boostrapped probability of overlap for the full distributions test.dispRity(disparity_centroids, test = bhatt.coeff, concatenate = FALSE) ## bhatt.coeff 2.5% 25% 75% 97.5% ## 120 : 80 0.2534144 0.0000000 0.1450953 0.3554093 0.5615906 ## 120 : 40 0.2726082 0.0000000 0.1632993 0.3942394 0.6435771 ## 120 : 0 0.2172550 0.0000000 0.0000000 0.3464102 0.5913591 ## 80 : 40 0.6016956 0.3097953 0.4779726 0.7208207 0.8306227 ## 80 : 0 0.4770117 0.1382736 0.3964076 0.5772107 0.7169318 ## 40 : 0 0.5327868 0.1724549 0.3887699 0.6761873 0.8781747 These results show the median overlap among pairs of distributions in the first column (bhatt.coeff) and then the distribution of these overlaps among each pair of bootstraps. In other words, when two distributions are compared, they are now compared for each bootstrap pseudo-replicate, thus effectively creating a distribution of probabilities of overlap. For example, when comparing the slices 120 and 80, we have a mean probability of overlap of 0.28 and a probability between 0.18 and 0.43 in 50% of the pseudo-replicates. Note that the quantiles and central tendencies can be modified via the conc.quantiles option. 4.8 Ecological studies of disparity Disparity analyses are not restricted to palaeobiology analysis. This package is designed for dealing with ordinated matrices, no matter which data was used in the ordination. Here is an short example of disparity analysis using ecological data from Deirdre McClean (unpubl.). The dataset contains an ordinated matrix of 20 dimensions (columns) for 40 elements (rows). The elements are different experimental plots with discrete variations of nutrient enrichment, depth, and freshwater benthic invertebrates. ## Loading the data data(McClean_data) ## Ordinated matrix ordinated_matrix &lt;- McClean_data$ordination ## Treatments and depths treatments &lt;- McClean_data$treatment depth &lt;- McClean_data$depth 4.8.1 Classic ecological “disparity” analysis A classical way to represent this ordinated data would be to use two dimensional plots to look at how the different experimental plots differ in their values for nutrient enrichment, depth, and freshwater benthic invertebrates. ## Setting the colors that will represent the treatments cols &lt;- sub(&quot;a&quot;, &quot;red&quot;, treatments) cols &lt;- sub(&quot;b&quot;, &quot;blue&quot;, cols) ## Setting the symbols that will represent the depth pchs &lt;- sub(1, 16, depth) pchs &lt;- as.numeric(sub(2, 17, pchs)) ## Graphical option op &lt;- par(bty = &quot;n&quot;) ## A classic 2D ordination plot plot(ordinated_matrix[, 1:2], col = cols, pch = pchs, xlab = &quot;PC 1&quot;, ylab = &quot;PC 2&quot;, xlim = range(ordinated_matrix[, 1:2]) + c(0, 100)) par(op) This shows the distribution of the experimental plots along the two first axis of variation of the ordinated distance matrix (i.e. the first two dimensions). At a first glance, it seems difficult to see a clear effect of the treatments (blue or red) or the depth (rounds or triangles) as the different experimental plots with the same parameters don’t seem to cluster together. However, this may be because this plot ignores the 18 other dimensions of the ordination! Additionally, these two represented dimensions do not represent a biological reality per se; i.e. the values on the first dimension do not represent a continuous trait (e.g. depth), instead they just represent the ordinations of correlations between the data and some factors. Therefore, we might want to approach this problem without getting stuck in only two dimensions and consider the whole dataset as a n-dimensional object. 4.8.2 Multidimensional analysis with dispRity 4.8.2.1 Splitting the data with respect to the different factors The first step is to create different subsamples that represent subsamples of the ordinated space (i.e. sub-regions within the n-dimensional object). Each of these subsamples will contain a certain number of elements (i.e. a subset of the 40 experimental field plots) that have some attributes in common. In our example, we are going to group the elements according to their depth and treatment. ## Creating a table that contain the elements and their attributes factors &lt;- as.data.frame(matrix(data = c(treatments, depth), nrow = nrow(ordinated_matrix), ncol = 2, byrow = FALSE, dimnames = list(rownames(ordinated_matrix)))) names(factors) &lt;- c(&quot;Treat&quot;, &quot;Depth&quot;) head(factors) ## Treat Depth ## 1 a 1 ## 1.1 a 2 ## 2 b 1 ## 2.1 b 2 ## 3 a 1 ## 3.1 a 1 Second, let’s split the data according by depth and treatment to create the subsamples of the ordinated space by using the custom.subsamples function: ## Splitting the ordinated space into four subsamples (customised_subsamples &lt;- custom.subsamples(ordinated_matrix, factors)) ## ---- dispRity object ---- ## 4 customised subsamples for 40 elements: ## Treat.a, Treat.b, Depth.1, Depth.2. 4.8.2.2 Calculating disparity In this example, we are going to define disparity as the n-dimensional ellipsoid volume (see ?hyper.volume). ## Calculating disparity as the ellipsoid volume of each group disparity &lt;- dispRity(customised_subsamples, metric = diagonal) summary(disparity) ## subsamples n obs ## 1 Treat.a 21 36.89 ## 2 Treat.b 19 40.99 ## 3 Depth.1 23 36.38 ## 4 Depth.2 17 40.74 We can also bootstrap the data to test the robustness of the measured disparity to outliers. Also, as we can see, each subsample has different numbers of elements. It might also be interesting to rarefy the data to have only subsamples with the same number of elements. ## Bootstrapping the subsamples and using rarefaction ## (i.e. only re-sampling 17 elements each time) bootstrapped_data &lt;- boot.matrix(customised_subsamples, bootstraps = 100, rarefaction = 17) We can now run a more robust disparity analysis using the bootstrapped data: ## Calculating the bootstrapped disparity disparity &lt;- dispRity(bootstrapped_data, metric = diagonal) 4.8.2.3 Summarising and displaying the results Now let’s summarise this data: ## Summary table of the data summary(disparity) ## subsamples n obs bs.median 2.5% 25% 75% 97.5% ## 1 Treat.a 21 36.89 35.01 31.32 32.88 35.44 36.11 ## 2 Treat.a 17 NA 33.69 30.38 31.91 34.97 35.98 ## 3 Treat.b 19 40.99 38.19 34.27 36.75 39.25 40.12 ## 4 Treat.b 17 NA 37.52 33.38 36.08 38.85 39.80 ## 5 Depth.1 23 36.38 34.44 31.86 33.63 35.01 35.52 ## 6 Depth.1 17 NA 33.38 30.56 32.28 34.09 35.18 ## 7 Depth.2 17 40.74 37.30 34.26 36.35 38.62 39.74 ## Graphical options op &lt;- par(mfrow = (c(1, 2)), bty = &quot;n&quot;) ## Plotting the bootstrapped disparity plot(disparity, main = &quot;Bootstrapped&quot;, las = 2, xlab = &quot;&quot;) ## Plotting the rarefied disparity plot(disparity, rarefaction = 17, main = &quot;Rarefied&quot;, las = 2, xlab = &quot;&quot;) par(op) As we can see, there seems to be no strong effect of the number of experimental plots in each subsample (i.e. the rarefied plot is really similar to the bootstrapped plot) which is a good thing! 4.8.2.4 Testing our hypotheses Finally, we can test our hypothesis (whether the water treatment at certain depths alters invertebrate communities and composition in natural habitats) by using the test.dispRity function. ## Testing the effect of treatment and depth on the bootstrapped data summary(test.dispRity(disparity, test = aov, comparisons = &quot;all&quot;)) ## Length Class Mode ## [1,] 13 aov list 4.9 Simulating discrete morphological data The dispRity package also allows users to simulate discrete morphological data matrices. In brief, the function sim.morpho takes a phylogenetic tree, the number of required characters, the evolutionary model, and a function from which to draw the rates. The package also contains a function for quickly checking the matrix’s phylogenetic signal (as defined in systematics not phylogenetic comparative methods) using parsimony. To understand these methods please refer to the phylogenetics literature. set.seed(3) ## Simulating a starting tree with 15 taxa as a random coalescent tree my_tree &lt;- rcoal(15) ## Generating a matrix with 100 characters (85% binary and 15% three state) and ## an equal rates model with a gamma rate distribution (0.5, 1) with no ## invariant characters. my_matrix &lt;- sim.morpho(tree = my_tree, characters = 100, states = c(0.85, 0.15), rates = c(rgamma, 0.5, 1), invariant = FALSE) ## The first few lines of the matrix my_matrix[1:5, 1:10] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## t15 &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;1&quot; &quot;1&quot; &quot;2&quot; &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; ## t12 &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;1&quot; &quot;1&quot; &quot;2&quot; &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; ## t14 &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;1&quot; &quot;1&quot; &quot;2&quot; &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; ## t6 &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;1&quot; &quot;1&quot; &quot;2&quot; &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; ## t3 &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;2&quot; &quot;1&quot; &quot;0&quot; ## Checking the matrix properties with a quick Maximum Parsimony tree search check.morpho(my_matrix, my_tree) ## ## Maximum parsimony 139.0000000 ## Consistency index 0.7625899 ## Retention index 0.8881356 ## Robinson-Foulds distance 0.0000000 Note that this example produces a tree with a great consistency index and an identical topology to the random coalescent tree! Nearly too good to be true… 4.9.1 A more detailed description sim.morpho is really flexible and takes many different arguments to allow users to simulate realistic matrices. It has three implemented models: &quot;ER&quot; for Equal Rates (the Mk model); &quot;HKY&quot;, the molecular HKY model but transforms pyrines in 0’s and pyrimidines in 1’s; or the &quot;mixed&quot; model that randomly uses an &quot;ER&quot; and/or &quot;HKY&quot; for the binary characters and &quot;ER&quot; for the multistate (&gt;2) characters. Both models take specific distributions for their rate or substitution models. These distributions should be passed to these arguments in the format of c(sampler_function, distribution_parameters) where the the sampler function is a the random generation function of that distribution (e.g. rnorm, runif, etc…) and the parameters are any parameters to be passed to this function. check.morpho runs a quick Maximum Parsimony tree search using the phangorn parsimony algorithm. It quickly calculates the parsimony score, the consistency and retention indices and, if a tree is provided (e.g. the tree used to generate the matrix) it calculates the Robinson-Foulds distance between the most parsimonious tree and the provided tree to determine how different they are. 4.9.2 Parameters for a realistic(ish) matrix There are many parameters that can create a realistic'' matrix (i.e. not too different from the input tree with a consistency and retention index close to what is seen in the literature) but because of the randomness of the matrix generation not all end up creating &quot;good&quot; matrices. The following parameters however, seem to generate fairlyrealistic’’ matrices with a starting coalescent tree, equal rates model with 0.85 binary characters and 0.15 three state characters, a gamma distribution with a shape parameter (\\(\\alpha\\)) of 5 and no scaling (\\(\\beta\\) = 1) with a rate of 100. set.seed(0) ## tree my_tree &lt;- rcoal(15) ## matrix morpho_mat &lt;- sim.morpho(my_tree, characters = 100, model = &quot;ER&quot;, rates = c(rgamma, rate = 100, shape = 5), invariant = FALSE) check.morpho(morpho_mat, my_tree) ## ## Maximum parsimony 104.0000000 ## Consistency index 0.0000000 ## Retention index 0.7886179 ## Robinson-Foulds distance 0.0000000 4.9.3 space.maker Another way to simulate data is to directly simulate an ordinated space with the space.maker function. This function allows users to simulate multidimensional spaces with a certain number of properties. It takes as arguments the number of elements (data points) and dimensions to create the space but also permits more fine tuning in the data simulation: it is possible to give a specific distribution to each dimension, provide a correlation matrix to link the dimensions or even provide an a priori distribution of the variance for each distributions! ## Graphical options op &lt;- par(mfrow = (c(2, 2)), bty = &quot;n&quot;) ## Visualising 3D spaces require(scatterplot3d) ## A cube space scatterplot3d(space.maker(2500, 3, runif), pch = 20, xlab = &quot;&quot;, ylab = &quot;&quot;, zlab = &quot;&quot;, main = &quot;Uniform cube space&quot;) ## A plane space scatterplot3d(space.maker(2500, 3, c(runif, runif, runif), arguments = list(list(min = 0, max = 0), NULL, NULL)), pch = 20, xlab = &quot;&quot;, ylab = &quot;&quot;, zlab = &quot;&quot;, main = &quot;Uniform plane space&quot;) ## An ellipsoid space (=a spheric space with correlation) cor_matrix &lt;- matrix(cbind(1, 0.8, 0.2, 0.8, 1, 0.7, 0.2, 0.7, 1), nrow = 3) scatterplot3d(space.maker(2500, 3, rnorm, cor.matrix = cor_matrix), pch = 20, xlab = &quot;&quot;, ylab = &quot;&quot;, zlab = &quot;&quot;, main = &quot;Normal ellipsoid space&quot;) ## A cylindrical space with a decreasing variance per axis scatterplot3d(space.maker(2500, 3, c(rnorm, rnorm, runif), scree = c(0.7, 0.2, 0.1)), pch = 20, xlab = &quot;&quot;, ylab = &quot;&quot;, zlab = &quot;&quot;, main = &quot;Normal cylindrical space&quot;) ## Resetting the graphic parameters par(op) "],
["the-guts-of-the-disprity-package.html", "5 The guts of the dispRity package 5.1 Manipulating dispRity objects 5.2 dispRity utilities", " 5 The guts of the dispRity package 5.1 Manipulating dispRity objects Disparity analysis involves a lot of manipulation of many matrices (especially when bootstrapping) which can be impractical to visualise and will quickly overwhelm your R console. Even the simple Beck and Lee 2014 example above produces an object with &gt; 72 lines of lists of lists of matrices! Therefore dispRity uses a specific class of object called a dispRity object. These objects allow users to use S3 method functions such as summary.dispRity, plot.dispRity and print.dispRity. dispRity also contains various utility functions that manipulate the dispRity object (e.g. sort.dispRity, extract.dispRity see the full list in the next section). These functions modify the dispRity object without having to delve into its complex structure! The full structure of a dispRity object is detailed here. ## Loading the example data data(disparity) ## What is the class of the median_centroids object? class(disparity) ## [1] &quot;dispRity&quot; ## What does the object contain? names(disparity) ## [1] &quot;matrix&quot; &quot;call&quot; &quot;subsamples&quot; &quot;disparity&quot; ## Summarising it using the S3 method print.dispRity disparity ## ---- dispRity object ---- ## 7 continuous (acctran) time subsamples for 99 elements with 97 dimensions: ## 90, 80, 70, 60, 50 ... ## Data was bootstrapped 100 times (method:&quot;full&quot;) and rarefied to 20, 15, 10, 5 elements. ## Disparity was calculated as: c(median, centroids). Note that it is always possible to recall the full object using the argument all = TRUE in print.dispRity: ## Display the full object print(disparity, all = TRUE) ## This is more nearly ~ 5000 lines on my 13 inch laptop screen! 5.2 dispRity utilities The package also provides some utility functions to facilitate multidimensional analysis. 5.2.1 dispRity object utilities The first set of utilities are functions for manipulating dispRity objects: 5.2.1.1 make.dispRity This function creates empty dispRity objects. ## Creating an empty dispRity object make.dispRity() ## Empty dispRity object. ## Creating an &quot;empty&quot; dispRity object with a matrix (disparity_obj &lt;- make.dispRity(matrix(rnorm(20), 5, 4))) ## Empty dispRity object. 5.2.1.2 fill.dispRity This function initialises a dispRity object and generates its call properties. ## The dispRity object&#39;s call is indeed empty disparity_obj$call ## list() ## Filling an empty disparity object (that needs to contain at least a matrix) (disparity_obj &lt;- fill.dispRity(disparity_obj)) ## ---- dispRity object ---- ## 5 elements with 4 dimensions. ## The dipRity object has now the correct minimal attributes disparity_obj$call ## $dimensions ## [1] 4 5.2.1.3 matrix.dispRity This function extracts a specific matrix from a disparity object. The matrix can be one of the bootstrapped matrices or/and a rarefied matrix. ## Extracting the matrix containing the coordinates of the elements at time 50 str(matrix.dispRity(disparity, &quot;50&quot;)) ## num [1:18, 1:97] -0.1038 0.2844 0.2848 0.0927 0.1619 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:18] &quot;Leptictis&quot; &quot;Dasypodidae&quot; &quot;n24&quot; &quot;Potamogalinae&quot; ... ## ..$ : NULL ## Extracting the 3rd bootstrapped matrix with the 2nd rarefaction level ## (15 elements) from the second group (80 Mya) str(matrix.dispRity(disparity, subsamples = 1, bootstrap = 3, rarefaction = 2)) ## num [1:15, 1:97] -0.7161 0.3496 -0.573 -0.0445 -0.1427 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:15] &quot;n7&quot; &quot;n34&quot; &quot;Maelestes&quot; &quot;n20&quot; ... ## ..$ : NULL 5.2.1.4 get.subsamples.dispRity This function creates a dispRity object that contains only elements from one specific subsamples. ## Extracting all the data for the crown mammals (crown_mammals &lt;- get.subsamples.dispRity(disp_crown_stemBS, &quot;Group.crown&quot;)) ## The object keeps the properties of the parent object but is composed of only one subsamples length(crown_mammals$subsamples) 5.2.1.5 extract.dispRity This function extracts the calculated disparity values of a specific matrix. ## Extracting the observed disparity (default) extract.dispRity(disparity) ## Extracting the disparity from the bootstrapped values from the ## 10th rarefaction level from the second subsamples (80 Mya) extract.dispRity(disparity, observed = FALSE, subsamples = 2, rarefaction = 10) 5.2.1.6 scale.dispRity This is the S3 method of scale (scaling and/or centring) that can be applied to the disparity data of a dispRity object. ## Getting the disparity values of the time subsamples head(summary(disparity)) ## Scaling the same disparity values head(summary(scale(disparity, scale = TRUE))) ## Scaling and centering: head(summary(scale(disparity, scale = TRUE, center = TRUE))) 5.2.1.7 sort.dispRity This is the S3 method of sort for sorting the subsamples alphabetically (default) or following a specific pattern. ## Sorting the disparity subsamples in inverse alphabetic order head(summary(sort(disparity, decreasing = TRUE))) ## Customised sorting head(summary(sort(disparity, sort = c(7, 1, 3, 4, 5, 2, 6)))) "],
["palaeobiology-demo-disparity-through-time-and-within-groups.html", "6 Palaeobiology demo: disparity-through-time and within groups 6.1 Before starting 6.2 A disparity-through-time analysis 6.3 Testing differences", " 6 Palaeobiology demo: disparity-through-time and within groups This demo aims to give quick overview of the dispRity package (v.0.3) for palaeobiology analyses of disparity, including disparity through time analyses. Please refer to my GitHub page for other vignettes, namely the dispRity manual that explains the functions in more detail. In brief, this package allows users to easily perform disparity-through-time analysis. This type of analysis often starts by ordinating morphometric or cladistic data into a multidimensional object hereafter called the morphospace. One might be interested in studying how the occupancy of the morphospace has changed through time by measuring a summary metric of this morphospace through time, called disparity. Disparity can then be seen as a value or a distribution of values summarising the diversity of morphologies. This demo showcases a typical disparity-through-time analysis: we are going to test whether the disparity changed through time in a subset of eutherian mammals from the last 100 million years. 6.1 Before starting 6.1.1 Installing dispRity You can install this package easily if you are using the latest version of R and devtools. if(!require(devtools)) install.packages(&quot;devtools&quot;) install_github(&quot;TGuillerme/dispRity&quot;, ref = &quot;release&quot;) library(dispRity) 6.1.2 The morphospace In this example, we are going to use a subset of the data from [@beckancient2014]. This dataset contains an ordinated matrix of 50 discrete characters from mammals (BeckLee_mat50), another matrix of the same 50 mammals and the estimated discrete data characters of their descendants (thus 50 + 49 rows, BeckLee_mat99), a dataframe containing the ages of each taxon in the dataset (BeckLee_ages) and finally a phylogenetic tree with the relationships among the 50 mammals (BeckLee_tree). The ordinated matrix will represent our full morphospace, i.e. all the mammalian morphologies that ever existed through time (for this dataset). ## Loading demo and the package data library(dispRity) ## Setting the random seed for repeatability set.seed(123) ## Loading the ordinated matrix/morphospace: data(BeckLee_mat50) head(BeckLee_mat50[,1:5]) ## [,1] [,2] [,3] [,4] [,5] ## Cimolestes -0.5319679 0.1117759259 0.09865194 -0.1933148 0.2035833 ## Maelestes -0.4087147 0.0139690317 0.26268300 0.2297096 0.1310953 ## Batodon -0.6923194 0.3308625215 -0.10175223 -0.1899656 0.1003108 ## Bulaklestes -0.6802291 -0.0134872777 0.11018009 -0.4103588 0.4326298 ## Daulestes -0.7386111 0.0009001369 0.12006449 -0.4978191 0.4741342 ## Uchkudukodon -0.5105254 -0.2420633915 0.44170317 -0.1172972 0.3602273 dim(BeckLee_mat50) ## [1] 50 48 ## The morphospace contains 50 taxa and has 48 dimensions (or axes) ## Showing a list of first and last occurrences data for some fossils data(BeckLee_ages) head(BeckLee_ages) ## FAD LAD ## Adapis 37.2 36.8 ## Asioryctes 83.6 72.1 ## Leptictis 33.9 33.3 ## Miacis 49.0 46.7 ## Mimotona 61.6 59.2 ## Notharctus 50.2 47.0 ## Plotting a phylogeny data(BeckLee_tree) plot(BeckLee_tree, cex = 0.7) axisPhylo(root = 140) You can have an even nicer looking tree if you use the strap package! if(!require(strap)) install.packages(&quot;strap&quot;) library(strap) geoscalePhylo(BeckLee_tree, cex.tip = 0.7, cex.ts = 0.6) 6.2 A disparity-through-time analysis 6.2.1 Splitting the morphospace through time One of the crucial steps in disparity-through-time analysis is to split the full morphospace into smaller time subsamples that contain the total number of morphologies at certain points in time (time-slicing) or during certain periods in time (time-binning). Basically, the full morphospace represents the total number of morphologies across all time and will be greater than any of the time subsamples of the morphospace. The dispRity package provides a time.subsamples function that allows users to split the morphospace into time slices (using method = continuous) or into time bins (using method = discrete). In this example, we are going to split the morphospace into five equal time bins of 20 million years long from 100 million years ago to the present. We will also provide to the function a table containing the first and last occurrences dates for some fossils to take into account that some fossils might occur in several of our different time bins. ## Creating the vector of time bins ages (time_bins &lt;- rev(seq(from = 0, to = 100, by = 20))) ## [1] 100 80 60 40 20 0 ## Splitting the morphospace using the time.subsamples function (binned_morphospace &lt;- time.subsamples(data = BeckLee_mat50, tree = BeckLee_tree, method = &quot;discrete&quot;, time = time_bins, inc.nodes = FALSE, FADLAD = BeckLee_ages)) ## ---- dispRity object ---- ## 5 discrete time subsamples for 50 elements: ## 100 - 80, 80 - 60, 60 - 40, 40 - 20, 20 - 0. The output object is a dispRity object. For details about this object class, please refer to the dispRity manual. In brief, however, dispRity objects are lists of different elements (i.e. disparity results, morphospace time subsamples, morphospace attributes, etc.) that display only a summary of the object when calling the object to avoiding filling the R console with superfluous output. ## Printing the class of the object class(binned_morphospace) ## [1] &quot;dispRity&quot; ## Printing the content of the object str(binned_morphospace) ## List of 3 ## $ matrix : num [1:50, 1:48] -0.532 -0.409 -0.692 -0.68 -0.739 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:50] &quot;Cimolestes&quot; &quot;Maelestes&quot; &quot;Batodon&quot; &quot;Bulaklestes&quot; ... ## .. ..$ : NULL ## $ call :List of 1 ## ..$ subsamples: chr &quot;discrete&quot; ## $ subsamples:List of 5 ## ..$ 100 - 80:List of 1 ## .. ..$ elements: int [1:8, 1] 5 4 6 8 43 10 11 42 ## ..$ 80 - 60 :List of 1 ## .. ..$ elements: int [1:15, 1] 7 8 9 1 2 3 12 13 14 44 ... ## ..$ 60 - 40 :List of 1 ## .. ..$ elements: int [1:13, 1] 41 49 24 25 26 27 28 21 22 19 ... ## ..$ 40 - 20 :List of 1 ## .. ..$ elements: int [1:6, 1] 15 39 40 35 23 47 ## ..$ 20 - 0 :List of 1 ## .. ..$ elements: int [1:10, 1] 36 37 38 32 33 34 50 48 29 30 ## - attr(*, &quot;class&quot;)= chr &quot;dispRity&quot; names(binned_morphospace) ## [1] &quot;matrix&quot; &quot;call&quot; &quot;subsamples&quot; ## Printing the object as a dispRity class binned_morphospace ## ---- dispRity object ---- ## 5 discrete time subsamples for 50 elements: ## 100 - 80, 80 - 60, 60 - 40, 40 - 20, 20 - 0. These objects will gradually contain more information when completing the following steps in the disparity-through-time analysis. 6.2.2 Bootstrapping the data Once we obtain our different time subsamples, we can bootstrap and rarefy them (i.e. pseudo-replicating the data). The bootstrapping allows us to make each subsample more robust to outliers and the rarefaction allows us to compare subsamples with the same number of taxa to remove sampling biases (i.e. more taxa in one subsample than the others). The boot.matrix function bootstraps the dispRity object and the rarefaction option within performs rarefaction. ## Bootstrapping each time subsample 100 times (boot_bin_morphospace &lt;- boot.matrix(binned_morphospace, bootstraps = 100)) ## ---- dispRity object ---- ## 5 discrete time subsamples for 50 elements with 48 dimensions: ## 100 - 80, 80 - 60, 60 - 40, 40 - 20, 20 - 0. ## Data was bootstrapped 100 times (method:&quot;full&quot;). ## Getting the minimum number of rows (i.e. taxa) in the time subsamples min(unlist(lapply(boot_bin_morphospace$subsamples, lapply, nrow))) ## [1] 6 ## Bootstrapping each time subsample 100 times and rarefying them (rare_bin_morphospace &lt;- boot.matrix(binned_morphospace, bootstraps = 100, rarefaction = 6)) ## ---- dispRity object ---- ## 5 discrete time subsamples for 50 elements with 48 dimensions: ## 100 - 80, 80 - 60, 60 - 40, 40 - 20, 20 - 0. ## Data was bootstrapped 100 times (method:&quot;full&quot;) and rarefied to 6 elements. 6.2.3 Calculating disparity We can now calculate the disparity within each time subsamples along with some confidence intervals generated by the pseudoreplication step above (bootstraps/rarefaction). Disparity can be calculated in many ways and this package allows users to come up with their own disparity metrics. For more details, please refer to the dispRity metric vignette. In this example, we are going to calculate the spread of the data in each time subsample by calculating disparity as the sum of the variance of each dimension of the morphospace in each time subsample using the dispRity function. Thus, in this example, disparity is defined by the multi-dimensional variance of each time subsample (i.e. the spread of the taxa within the morphospace). Note that this metric comes with a caveat (not solved here) since it ignores covariances among the dimensions of the morphospace. We use this here because it is a standard metric used in disparity-through-time analysis. ## Calculating disparity for the bootstrapped data (boot_disparity &lt;- dispRity(boot_bin_morphospace, metric = c(sum, variances))) ## ---- dispRity object ---- ## 5 discrete time subsamples for 50 elements with 48 dimensions: ## 100 - 80, 80 - 60, 60 - 40, 40 - 20, 20 - 0. ## Data was bootstrapped 100 times (method:&quot;full&quot;). ## Disparity was calculated as: c(sum, variances). ## Calculating disparity for the rarefied data (rare_disparity &lt;- dispRity(rare_bin_morphospace, metric = c(sum, variances))) ## ---- dispRity object ---- ## 5 discrete time subsamples for 50 elements with 48 dimensions: ## 100 - 80, 80 - 60, 60 - 40, 40 - 20, 20 - 0. ## Data was bootstrapped 100 times (method:&quot;full&quot;) and rarefied to 6 elements. ## Disparity was calculated as: c(sum, variances). The dispRity function does not actually display the calculated disparity values but rather only the properties of the disparity object (size, subsamples, metric, etc.). To display the actual calculated scores, we need to summarise the disparity object using the S3 method summary that is applied to a dispRity object (see ?summary.dispRity for more details). As for any R package, you can refer to the help files for each individual function for more details. ## Summarising the disparity results summary(boot_disparity) ## subsamples n obs bs.median 2.5% 25% 75% 97.5% ## 1 100 - 80 8 1.675 1.488 1.087 1.389 1.568 1.648 ## 2 80 - 60 15 1.782 1.679 1.538 1.631 1.728 1.792 ## 3 60 - 40 13 1.913 1.772 1.607 1.734 1.826 1.886 ## 4 40 - 20 6 2.022 1.707 1.212 1.537 1.822 1.942 ## 5 20 - 0 10 1.971 1.794 1.598 1.716 1.842 1.890 summary(rare_disparity) ## subsamples n obs bs.median 2.5% 25% 75% 97.5% ## 1 100 - 80 8 1.675 1.484 1.194 1.400 1.547 1.636 ## 2 100 - 80 6 NA 1.477 0.993 1.361 1.569 1.698 ## 3 80 - 60 15 1.782 1.674 1.517 1.600 1.725 1.793 ## 4 80 - 60 6 NA 1.655 1.299 1.532 1.754 1.882 ## 5 60 - 40 13 1.913 1.767 1.601 1.714 1.829 1.861 ## 6 60 - 40 6 NA 1.787 1.314 1.672 1.879 1.984 ## 7 40 - 20 6 2.022 1.736 1.281 1.603 1.822 1.948 ## 8 20 - 0 10 1.971 1.807 1.595 1.729 1.856 1.917 ## 9 20 - 0 6 NA 1.790 1.435 1.718 1.873 1.995 The summary.dispRity function comes with many options on which values to calculate (central tendency and quantiles) and on how many digits to display. Refer to the function’s manual for more details. 6.2.4 Plotting the results It is sometimes easier to visualise the results in a plot than in a table. For that we can use the plot S3 function to plot the dispRity objects (see ?plot.dispRity for more details). ## Graphical options quartz(width = 10, height = 5) ; par(mfrow = (c(1,2)), bty = &quot;n&quot;) ## Plotting the bootstrapped and rarefied results plot(boot_disparity, type = &quot;continuous&quot;, main = &quot;bootstrapped results&quot;) plot(rare_disparity, type = &quot;continuous&quot;, main = &quot;rarefied results&quot;) 6.3 Testing differences Finally, to draw some valid conclusions from these results, we can apply some statistical tests. We can test, for example, if mammalian disparity changed significantly through time over the last 100 million years. To do so, we can compare the means of each time-bin in a sequential manner to see whether the disparity in bin n is equal to the disparity in bin n+1, and whether this is in turn equal to the disparity in bin n+2, etc. Because our data is temporally autocorrelated (i.e. what happens in bin n+1 depends on what happened in bin n) and pseudoreplicated (i.e. each bootstrap draw creates non-independent time subsamples because they are all based on the same time subsamples), we apply a non-parametric mean comparison: the wilcox.test. Also, we need to apply a p-value correction (e.g. Bonferroni correction) to correct for multiple testing (see ?p.adjust for more details). ## Testing the differences between bins in the bootstrapped dataset. test.dispRity(boot_disparity, test = wilcox.test, comparison = &quot;sequential&quot;, correction = &quot;bonferroni&quot;) ## [[1]] ## statistic ## 100 - 80 : 80 - 60 471 ## 80 - 60 : 60 - 40 1562 ## 60 - 40 : 40 - 20 6250 ## 40 - 20 : 20 - 0 3725 ## ## [[2]] ## p.value ## 100 - 80 : 80 - 60 7.427563e-28 ## 80 - 60 : 60 - 40 1.798899e-16 ## 60 - 40 : 40 - 20 9.061511e-03 ## 40 - 20 : 20 - 0 7.379715e-03 ## Testing the differences between bins in the rarefied dataset. test.dispRity(rare_disparity, test = wilcox.test, comparison = &quot;sequential&quot;, correction = &quot;bonferroni&quot;) ## [[1]] ## statistic ## 100 - 80 : 80 - 60 662 ## 80 - 60 : 60 - 40 1814 ## 60 - 40 : 40 - 20 5752 ## 40 - 20 : 20 - 0 3621 ## ## [[2]] ## p.value ## 100 - 80 : 80 - 60 1.214988e-25 ## 80 - 60 : 60 - 40 2.823697e-14 ## 60 - 40 : 40 - 20 2.653018e-01 ## 40 - 20 : 20 - 0 3.026079e-03 Here our results show significant changes in disparity through time between all time bins (all p-values &lt; 0.05). However, when looking at the rarefied results, there is no significant difference between the time bins in the Palaeogene (60-40 to 40-20 Mya), suggesting that the differences detected in the first test might just be due to the differences in number of taxa sampled (13 or 6 taxa) in each time bin. 6.3.1 References "]
]
